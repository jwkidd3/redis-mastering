{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5: Advanced CLI Operations for Insurance Monitoring\n",
    "## Python Alternative Implementation\n",
    "### Duration: 45 minutes\n",
    "\n",
    "This lab covers advanced Redis monitoring techniques for insurance systems using Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Initial Connection\n",
    "\n",
    "First, let's import the required libraries and establish a connection to Redis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import redis\n",
    "import time\n",
    "import json\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Redis\n",
    "r = redis.Redis(host='localhost', port=6379, decode_responses=True)\n",
    "\n",
    "# Test connection\n",
    "try:\n",
    "    r.ping()\n",
    "    print(\"✅ Redis connection successful!\")\n",
    "    print(f\"Redis server version: {r.info()['redis_version']}\")\n",
    "except redis.ConnectionError:\n",
    "    print(\"❌ Failed to connect to Redis. Please ensure Redis is running on localhost:6379\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Configure Redis for Advanced Monitoring\n",
    "\n",
    "Configure Redis settings for monitoring slow queries and latency issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure slow query logging\n",
    "r.config_set('slowlog-log-slower-than', 10000)  # Log queries slower than 10ms\n",
    "r.config_set('slowlog-max-len', 128)  # Keep last 128 slow queries\n",
    "\n",
    "# Configure latency monitoring\n",
    "r.config_set('latency-monitor-threshold', 100)  # Monitor latencies > 100ms\n",
    "\n",
    "# Display current configuration\n",
    "slowlog_config = r.config_get('slowlog*')\n",
    "latency_config = r.config_get('latency*')\n",
    "\n",
    "print(\"📊 Redis Monitoring Configuration:\")\n",
    "print(\"\\nSlow Query Settings:\")\n",
    "for key, value in slowlog_config.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\nLatency Monitoring Settings:\")\n",
    "for key, value in latency_config.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: System Metrics Collection\n",
    "\n",
    "Create a comprehensive monitoring system for insurance operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InsuranceMonitor:\n",
    "    \"\"\"Advanced monitoring system for insurance Redis operations\"\"\"\n",
    "    \n",
    "    def __init__(self, redis_client):\n",
    "        self.redis = redis_client\n",
    "        self.start_time = time.time()\n",
    "    \n",
    "    def get_system_metrics(self) -> Dict[str, Any]:\n",
    "        \"\"\"Collect comprehensive system metrics\"\"\"\n",
    "        info = self.redis.info()\n",
    "        \n",
    "        metrics = {\n",
    "            'Server': {\n",
    "                'version': info.get('redis_version', 'N/A'),\n",
    "                'uptime_days': info.get('uptime_in_days', 0),\n",
    "                'config_file': info.get('config_file', 'N/A')\n",
    "            },\n",
    "            'Clients': {\n",
    "                'connected': info.get('connected_clients', 0),\n",
    "                'blocked': info.get('blocked_clients', 0),\n",
    "                'max_clients': info.get('maxclients', 'N/A')\n",
    "            },\n",
    "            'Memory': {\n",
    "                'used_memory': info.get('used_memory_human', 'N/A'),\n",
    "                'peak_memory': info.get('used_memory_peak_human', 'N/A'),\n",
    "                'memory_fragmentation': info.get('mem_fragmentation_ratio', 'N/A')\n",
    "            },\n",
    "            'Performance': {\n",
    "                'ops_per_sec': info.get('instantaneous_ops_per_sec', 0),\n",
    "                'total_commands': info.get('total_commands_processed', 0),\n",
    "                'keyspace_hits': info.get('keyspace_hits', 0),\n",
    "                'keyspace_misses': info.get('keyspace_misses', 0)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Calculate hit rate\n",
    "        hits = metrics['Performance']['keyspace_hits']\n",
    "        misses = metrics['Performance']['keyspace_misses']\n",
    "        if hits + misses > 0:\n",
    "            metrics['Performance']['hit_rate'] = f\"{(hits / (hits + misses)) * 100:.2f}%\"\n",
    "        else:\n",
    "            metrics['Performance']['hit_rate'] = \"N/A\"\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def get_insurance_key_stats(self) -> Dict[str, int]:\n",
    "        \"\"\"Get statistics for insurance-specific keys\"\"\"\n",
    "        patterns = {\n",
    "            'total_keys': self.redis.dbsize(),\n",
    "            'policies': len(list(self.redis.scan_iter('policy:*'))),\n",
    "            'customers': len(list(self.redis.scan_iter('customer:*'))),\n",
    "            'claims': len(list(self.redis.scan_iter('claim:*'))),\n",
    "            'quotes': len(list(self.redis.scan_iter('quote:*'))),\n",
    "            'agents': len(list(self.redis.scan_iter('agent:*')))\n",
    "        }\n",
    "        return patterns\n",
    "\n",
    "# Create monitor instance and display metrics\n",
    "monitor = InsuranceMonitor(r)\n",
    "metrics = monitor.get_system_metrics()\n",
    "\n",
    "print(\"📊 System Metrics Dashboard\")\n",
    "print(\"=\" * 50)\n",
    "for category, values in metrics.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for key, value in values.items():\n",
    "        print(f\"  {key:<20} {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get insurance-specific key statistics\n",
    "key_stats = monitor.get_insurance_key_stats()\n",
    "\n",
    "print(\"🔑 Insurance Key Statistics\")\n",
    "print(\"=\" * 50)\n",
    "for key_type, count in key_stats.items():\n",
    "    print(f\"  {key_type:<15} {count:>10,} keys\")\n",
    "\n",
    "# Create visualization\n",
    "if sum(key_stats.values()) > 0:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Bar chart\n",
    "    categories = list(key_stats.keys())[1:]  # Exclude total_keys\n",
    "    values = [key_stats[k] for k in categories]\n",
    "    ax1.bar(categories, values, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd'])\n",
    "    ax1.set_ylabel('Number of Keys')\n",
    "    ax1.set_title('Insurance Data Distribution')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Pie chart\n",
    "    non_zero = {k: v for k, v in key_stats.items() if v > 0 and k != 'total_keys'}\n",
    "    if non_zero:\n",
    "        ax2.pie(non_zero.values(), labels=non_zero.keys(), autopct='%1.1f%%', startangle=90)\n",
    "        ax2.set_title('Key Distribution by Type')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Performance Benchmarking\n",
    "\n",
    "Benchmark typical insurance operations to identify performance characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerformanceBenchmark:\n",
    "    \"\"\"Benchmark insurance operations\"\"\"\n",
    "    \n",
    "    def __init__(self, redis_client):\n",
    "        self.redis = redis_client\n",
    "        self.results = []\n",
    "    \n",
    "    def benchmark_operation(self, name: str, operation, iterations: int = 1000):\n",
    "        \"\"\"Benchmark a specific operation\"\"\"\n",
    "        print(f\"⚡ Benchmarking {name} ({iterations} iterations)...\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        for i in range(iterations):\n",
    "            operation(i)\n",
    "        duration = time.time() - start_time\n",
    "        \n",
    "        ops_per_sec = iterations / duration\n",
    "        avg_latency_ms = (duration / iterations) * 1000\n",
    "        \n",
    "        result = {\n",
    "            'operation': name,\n",
    "            'iterations': iterations,\n",
    "            'duration_sec': round(duration, 3),\n",
    "            'ops_per_sec': round(ops_per_sec, 0),\n",
    "            'avg_latency_ms': round(avg_latency_ms, 3)\n",
    "        }\n",
    "        \n",
    "        self.results.append(result)\n",
    "        print(f\"  ✓ {ops_per_sec:,.0f} ops/sec, {avg_latency_ms:.3f}ms avg latency\")\n",
    "        return result\n",
    "    \n",
    "    def run_insurance_benchmarks(self):\n",
    "        \"\"\"Run comprehensive insurance operation benchmarks\"\"\"\n",
    "        \n",
    "        # Policy lookup\n",
    "        self.benchmark_operation(\n",
    "            'Policy Lookup',\n",
    "            lambda i: self.redis.get(f'policy:AUTO-{100000 + (i % 100)}')\n",
    "        )\n",
    "        \n",
    "        # Customer batch retrieval\n",
    "        self.benchmark_operation(\n",
    "            'Customer Batch Get',\n",
    "            lambda i: self.redis.mget([f'customer:CUST{j:04d}' for j in range(i % 10, (i % 10) + 5)]),\n",
    "            iterations=500\n",
    "        )\n",
    "        \n",
    "        # Claims queue operations\n",
    "        self.benchmark_operation(\n",
    "            'Claims Queue',\n",
    "            lambda i: [self.redis.lpush('claims:benchmark', f'CLM-{i}'), \n",
    "                      self.redis.rpop('claims:benchmark')]\n",
    "        )\n",
    "        \n",
    "        # Premium calculations\n",
    "        self.benchmark_operation(\n",
    "            'Premium Updates',\n",
    "            lambda i: [self.redis.set(f'premium:test:{i}', 1000),\n",
    "                      self.redis.incrby(f'premium:test:{i}', 150),\n",
    "                      self.redis.delete(f'premium:test:{i}')]\n",
    "        )\n",
    "        \n",
    "        # Quote generation with TTL\n",
    "        self.benchmark_operation(\n",
    "            'Quote Generation',\n",
    "            lambda i: self.redis.setex(f'quote:BENCH:{i}', 300, json.dumps({\n",
    "                'quote_id': f'Q{i:06d}',\n",
    "                'premium': 1200 + (i % 500),\n",
    "                'coverage': 'comprehensive'\n",
    "            }))\n",
    "        )\n",
    "        \n",
    "        return pd.DataFrame(self.results)\n",
    "\n",
    "# Run benchmarks\n",
    "benchmark = PerformanceBenchmark(r)\n",
    "results_df = benchmark.run_insurance_benchmarks()\n",
    "\n",
    "print(\"\\n📊 Benchmark Results Summary\")\n",
    "print(\"=\" * 70)\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize benchmark results\n",
    "if len(results_df) > 0:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Operations per second\n",
    "    ax1 = axes[0, 0]\n",
    "    colors = plt.cm.viridis(range(len(results_df)))\n",
    "    ax1.bar(results_df['operation'], results_df['ops_per_sec'], color=colors)\n",
    "    ax1.set_ylabel('Operations/Second')\n",
    "    ax1.set_title('Throughput Comparison')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Average latency\n",
    "    ax2 = axes[0, 1]\n",
    "    ax2.bar(results_df['operation'], results_df['avg_latency_ms'], color=colors)\n",
    "    ax2.set_ylabel('Latency (ms)')\n",
    "    ax2.set_title('Average Latency per Operation')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Duration comparison\n",
    "    ax3 = axes[1, 0]\n",
    "    ax3.bar(results_df['operation'], results_df['duration_sec'], color=colors)\n",
    "    ax3.set_ylabel('Duration (seconds)')\n",
    "    ax3.set_title('Total Benchmark Duration')\n",
    "    ax3.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Performance comparison (normalized)\n",
    "    ax4 = axes[1, 1]\n",
    "    max_ops = results_df['ops_per_sec'].max()\n",
    "    normalized = (results_df['ops_per_sec'] / max_ops) * 100\n",
    "    ax4.barh(results_df['operation'], normalized, color=colors)\n",
    "    ax4.set_xlabel('Performance (% of max)')\n",
    "    ax4.set_title('Relative Performance')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Memory Analysis\n",
    "\n",
    "Analyze memory usage patterns for insurance data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_memory_usage(redis_client):\n",
    "    \"\"\"Analyze memory usage for different key types\"\"\"\n",
    "    \n",
    "    print(\"💾 Memory Usage Analysis\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Sample keys for memory analysis\n",
    "    sample_keys = [\n",
    "        ('policy:AUTO-100001', 'Auto Policy'),\n",
    "        ('customer:CUST001', 'Customer Profile'),\n",
    "        ('claim:CLM-2024-001', 'Claim Record'),\n",
    "        ('quote:AUTO:Q100001', 'Quote Data'),\n",
    "        ('agent:AG001', 'Agent Profile')\n",
    "    ]\n",
    "    \n",
    "    memory_data = []\n",
    "    \n",
    "    for key, description in sample_keys:\n",
    "        if redis_client.exists(key):\n",
    "            try:\n",
    "                # Get memory usage (Redis 4.0+)\n",
    "                usage = redis_client.memory_usage(key)\n",
    "                key_type = redis_client.type(key)\n",
    "                ttl = redis_client.ttl(key)\n",
    "                \n",
    "                memory_data.append({\n",
    "                    'key': key,\n",
    "                    'description': description,\n",
    "                    'type': key_type,\n",
    "                    'memory_bytes': usage,\n",
    "                    'ttl': 'No expiry' if ttl == -1 else f'{ttl}s'\n",
    "                })\n",
    "                \n",
    "                print(f\"  {description:<20} {usage:>8} bytes (TTL: {ttl})\")\n",
    "            except:\n",
    "                print(f\"  {description:<20} Memory usage not available\")\n",
    "        else:\n",
    "            print(f\"  {description:<20} Key not found\")\n",
    "    \n",
    "    # Get overall memory stats\n",
    "    try:\n",
    "        mem_stats = redis_client.memory_stats()\n",
    "        print(f\"\\n📊 Overall Memory Statistics:\")\n",
    "        print(f\"  Peak allocated: {mem_stats.get('peak.allocated', 'N/A')}\")\n",
    "        print(f\"  Total allocated: {mem_stats.get('total.allocated', 'N/A')}\")\n",
    "        print(f\"  Fragmentation ratio: {mem_stats.get('fragmentation', 'N/A')}\")\n",
    "    except:\n",
    "        info = redis_client.info('memory')\n",
    "        print(f\"\\n📊 Memory Info:\")\n",
    "        print(f\"  Used memory: {info.get('used_memory_human', 'N/A')}\")\n",
    "        print(f\"  Peak memory: {info.get('used_memory_peak_human', 'N/A')}\")\n",
    "        print(f\"  Fragmentation: {info.get('mem_fragmentation_ratio', 'N/A')}\")\n",
    "    \n",
    "    return memory_data\n",
    "\n",
    "# Analyze memory\n",
    "memory_analysis = analyze_memory_usage(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Slow Query Analysis\n",
    "\n",
    "Analyze slow queries to identify performance bottlenecks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_slow_queries(redis_client, num_queries=10):\n",
    "    \"\"\"Analyze recent slow queries\"\"\"\n",
    "    \n",
    "    print(f\"🐌 Analyzing Top {num_queries} Slow Queries\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    slow_queries = redis_client.slowlog_get(num_queries)\n",
    "    \n",
    "    if not slow_queries:\n",
    "        print(\"No slow queries found. Great performance!\")\n",
    "        return []\n",
    "    \n",
    "    query_data = []\n",
    "    \n",
    "    for i, query in enumerate(slow_queries, 1):\n",
    "        command = ' '.join(query['command'][:10])  # First 10 parts\n",
    "        if len(query['command']) > 10:\n",
    "            command += '...'\n",
    "        \n",
    "        duration_ms = query['duration'] / 1000\n",
    "        timestamp = datetime.fromtimestamp(query['start_time']).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "        query_data.append({\n",
    "            'rank': i,\n",
    "            'command': command[:50],  # Truncate long commands\n",
    "            'duration_ms': duration_ms,\n",
    "            'timestamp': timestamp\n",
    "        })\n",
    "        \n",
    "        print(f\"\\n  #{i} - {duration_ms:.2f}ms\")\n",
    "        print(f\"     Command: {command[:60]}\")\n",
    "        print(f\"     Time: {timestamp}\")\n",
    "    \n",
    "    return query_data\n",
    "\n",
    "# Analyze slow queries\n",
    "slow_query_data = analyze_slow_queries(r)\n",
    "\n",
    "if slow_query_data:\n",
    "    df_slow = pd.DataFrame(slow_query_data)\n",
    "    print(\"\\n📊 Slow Query Summary:\")\n",
    "    print(df_slow[['rank', 'duration_ms', 'command']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Real-time Claims Monitoring\n",
    "\n",
    "Monitor claims processing in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClaimsMonitor:\n",
    "    \"\"\"Real-time claims monitoring system\"\"\"\n",
    "    \n",
    "    def __init__(self, redis_client):\n",
    "        self.redis = redis_client\n",
    "        self.reset_metrics()\n",
    "    \n",
    "    def reset_metrics(self):\n",
    "        \"\"\"Reset daily metrics\"\"\"\n",
    "        self.redis.set('metrics:claims:submitted:today', 0)\n",
    "        self.redis.set('metrics:claims:processed:today', 0)\n",
    "        self.redis.set('metrics:claims:rejected:today', 0)\n",
    "        self.redis.set('metrics:claims:pending:today', 0)\n",
    "    \n",
    "    def simulate_claims(self, num_claims=30):\n",
    "        \"\"\"Simulate claims processing\"\"\"\n",
    "        import random\n",
    "        \n",
    "        print(f\"\\n🚀 Simulating {num_claims} insurance claims...\")\n",
    "        \n",
    "        for i in range(num_claims):\n",
    "            claim_id = f'CLM-2024-{i:06d}'\n",
    "            \n",
    "            # Submit claim\n",
    "            self.redis.lpush('claims:pending', claim_id)\n",
    "            self.redis.incr('metrics:claims:submitted:today')\n",
    "            \n",
    "            # Process with some delay\n",
    "            time.sleep(0.05)  # Simulate processing time\n",
    "            \n",
    "            # Process claim (70% approved, 30% rejected)\n",
    "            claim = self.redis.rpop('claims:pending')\n",
    "            if claim:\n",
    "                if random.random() < 0.7:\n",
    "                    self.redis.lpush('claims:approved', claim)\n",
    "                    self.redis.incr('metrics:claims:processed:today')\n",
    "                else:\n",
    "                    self.redis.lpush('claims:rejected', claim)\n",
    "                    self.redis.incr('metrics:claims:rejected:today')\n",
    "            \n",
    "            # Display progress every 10 claims\n",
    "            if (i + 1) % 10 == 0:\n",
    "                self.display_metrics()\n",
    "        \n",
    "        print(\"\\n✅ Simulation complete!\")\n",
    "        self.display_final_metrics()\n",
    "    \n",
    "    def display_metrics(self):\n",
    "        \"\"\"Display current metrics\"\"\"\n",
    "        submitted = int(self.redis.get('metrics:claims:submitted:today') or 0)\n",
    "        processed = int(self.redis.get('metrics:claims:processed:today') or 0)\n",
    "        rejected = int(self.redis.get('metrics:claims:rejected:today') or 0)\n",
    "        pending = self.redis.llen('claims:pending')\n",
    "        \n",
    "        print(f\"  📊 Claims - Submitted: {submitted}, Processed: {processed}, Rejected: {rejected}, Pending: {pending}\")\n",
    "    \n",
    "    def display_final_metrics(self):\n",
    "        \"\"\"Display final metrics with visualization\"\"\"\n",
    "        submitted = int(self.redis.get('metrics:claims:submitted:today') or 0)\n",
    "        processed = int(self.redis.get('metrics:claims:processed:today') or 0)\n",
    "        rejected = int(self.redis.get('metrics:claims:rejected:today') or 0)\n",
    "        pending = self.redis.llen('claims:pending')\n",
    "        \n",
    "        print(\"\\n📈 Final Claims Metrics:\")\n",
    "        print(\"=\" * 40)\n",
    "        print(f\"  Total Submitted: {submitted}\")\n",
    "        print(f\"  Approved: {processed} ({processed/submitted*100:.1f}%)\")\n",
    "        print(f\"  Rejected: {rejected} ({rejected/submitted*100:.1f}%)\")\n",
    "        print(f\"  Still Pending: {pending}\")\n",
    "        \n",
    "        # Create visualization\n",
    "        if submitted > 0:\n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "            \n",
    "            # Status distribution\n",
    "            statuses = ['Approved', 'Rejected', 'Pending']\n",
    "            counts = [processed, rejected, pending]\n",
    "            colors = ['#2ecc71', '#e74c3c', '#f39c12']\n",
    "            ax1.bar(statuses, counts, color=colors)\n",
    "            ax1.set_ylabel('Number of Claims')\n",
    "            ax1.set_title('Claims Status Distribution')\n",
    "            \n",
    "            # Pie chart\n",
    "            ax2.pie(counts, labels=statuses, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "            ax2.set_title('Claims Processing Breakdown')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "# Create claims monitor and run simulation\n",
    "claims_monitor = ClaimsMonitor(r)\n",
    "claims_monitor.simulate_claims(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Key Pattern Analysis\n",
    "\n",
    "Analyze key patterns and identify potential optimization opportunities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_key_patterns(redis_client):\n",
    "    \"\"\"Analyze key naming patterns and distribution\"\"\"\n",
    "    \n",
    "    print(\"🔍 Key Pattern Analysis\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    patterns = {\n",
    "        'policy:*': 'Policies',\n",
    "        'customer:*': 'Customers',\n",
    "        'claim:*': 'Claims',\n",
    "        'quote:*': 'Quotes',\n",
    "        'agent:*': 'Agents',\n",
    "        'metrics:*': 'Metrics',\n",
    "        'premium:*': 'Premiums',\n",
    "        'claims:*': 'Claim Queues'\n",
    "    }\n",
    "    \n",
    "    pattern_counts = {}\n",
    "    \n",
    "    for pattern, description in patterns.items():\n",
    "        count = len(list(redis_client.scan_iter(pattern, count=100)))\n",
    "        pattern_counts[description] = count\n",
    "        print(f\"  {description:<15} {count:>8} keys\")\n",
    "    \n",
    "    total_keys = redis_client.dbsize()\n",
    "    identified_keys = sum(pattern_counts.values())\n",
    "    unidentified = total_keys - identified_keys\n",
    "    \n",
    "    print(f\"\\n  {'Total Keys':<15} {total_keys:>8}\")\n",
    "    print(f\"  {'Identified':<15} {identified_keys:>8} ({identified_keys/total_keys*100:.1f}%)\")\n",
    "    print(f\"  {'Unidentified':<15} {unidentified:>8} ({unidentified/total_keys*100:.1f}%)\")\n",
    "    \n",
    "    # Find largest keys\n",
    "    print(\"\\n📏 Top 5 Largest Keys:\")\n",
    "    key_sizes = []\n",
    "    \n",
    "    for key in redis_client.scan_iter(count=100):\n",
    "        try:\n",
    "            size = redis_client.memory_usage(key)\n",
    "            if size:\n",
    "                key_sizes.append((key, size))\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    key_sizes.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    for i, (key, size) in enumerate(key_sizes[:5], 1):\n",
    "        print(f\"  {i}. {key[:40]:<40} {size:>10,} bytes\")\n",
    "    \n",
    "    return pattern_counts\n",
    "\n",
    "# Analyze patterns\n",
    "pattern_analysis = analyze_key_patterns(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 5 Summary and Conclusions\n",
    "\n",
    "### ✅ Completed Tasks\n",
    "\n",
    "In this lab, you have successfully:\n",
    "\n",
    "1. **Configured Redis** for advanced monitoring (slow queries, latency)\n",
    "2. **Collected system metrics** including memory, performance, and client stats\n",
    "3. **Benchmarked insurance operations** to understand performance characteristics\n",
    "4. **Analyzed memory usage** patterns for different data types\n",
    "5. **Monitored slow queries** to identify bottlenecks\n",
    "6. **Simulated real-time claims processing** with metrics tracking\n",
    "7. **Analyzed key patterns** to understand data distribution\n",
    "\n",
    "### 📊 Key Performance Insights\n",
    "\n",
    "Based on the benchmarks and analysis:\n",
    "- Simple GET operations provide the highest throughput\n",
    "- Batch operations (MGET) are more efficient than multiple individual operations\n",
    "- Queue operations (LPUSH/RPOP) are suitable for high-volume claims processing\n",
    "- TTL-based operations (SETEX) add minimal overhead for quote management\n",
    "\n",
    "### 🎯 Best Practices for Production\n",
    "\n",
    "1. **Monitor continuously** - Set up dashboards for real-time monitoring\n",
    "2. **Analyze slow queries regularly** - Identify and optimize bottlenecks\n",
    "3. **Track memory usage** - Prevent memory issues before they occur\n",
    "4. **Use appropriate data structures** - Choose the right Redis type for each use case\n",
    "5. **Implement metrics collection** - Track business KPIs alongside technical metrics\n",
    "\n",
    "### 🚀 Next Steps\n",
    "\n",
    "- Implement automated alerting based on metrics thresholds\n",
    "- Create custom dashboards for insurance operations\n",
    "- Integrate monitoring with your application logging\n",
    "- Set up regular performance benchmarking\n",
    "- Explore Redis Sentinel for high availability monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final cleanup (optional)\n",
    "print(\"🧹 Cleaning up test data...\")\n",
    "\n",
    "# Clean up benchmark and test keys\n",
    "for key in r.scan_iter('premium:test:*'):\n",
    "    r.delete(key)\n",
    "for key in r.scan_iter('quote:BENCH:*'):\n",
    "    r.delete(key)\n",
    "\n",
    "# Clear test queues\n",
    "r.delete('claims:benchmark')\n",
    "\n",
    "print(\"✅ Lab 5 completed successfully!\")\n",
    "print(\"\\n📚 You are now ready to monitor Redis in production environments!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
